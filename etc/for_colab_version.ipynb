{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 아래 주소에서 실제 colab 실제 동작된 결과를 확인할 수 있습니다.\n",
    "\n",
    "- https://colab.research.google.com/drive/1pwoe7OlsfOBdVxHHaKtJDWIpJzs7KhhD?usp=sharing\n",
    "\n",
    "\n",
    "## 들여쓰기 등이 colab 기준으로 되어 있어서 현 notebook 코드에선 알맞게 되어 있지 않을 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium\n",
    "!pip install requests\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install lxml\n",
    "!pip install beautifulsoup4\n",
    "!pip install newspaper3k\n",
    "!pip install tqdm\n",
    "\n",
    "!apt-get update\n",
    "!apt install chromium-chromedriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import lxml\n",
    "import requests as r\n",
    "import datetime as d\n",
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from multiprocessing import Process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_driver():\n",
    "    #chrome option을 window가 없이도 돌아가도록 설정\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument('--headless')\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "    driver = webdriver.Chrome('chromedriver', chrome_options=chrome_options)\n",
    "    return driver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_list = ['bts', 'korea', 'ai']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_data(href_list, title_list, date_list, content_list):\n",
    "    print(\n",
    "        pd.DataFrame({\n",
    "            'href' : href_list,\n",
    "            'title' : title_list,\n",
    "            'date' : date_list,\n",
    "            'content' : content_list\n",
    "        }).head()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = set_driver()\n",
    "# search list를 하나씩 돌면서\n",
    "for q in search_list:\n",
    "  href_list, title_list, date_list, content_list = [], [], [], []\n",
    "  url = f\"https://edition.cnn.com/search/?size=10&q={q}&page=\"\n",
    "  # 아래 range를 변형해서 더 많은 데이터를 수집할 수 있음\n",
    "  for i in range(1, 4):\n",
    "    # 처음일 때와 아닐 때 url 값이 바뀌어서 flag를 이용해 변환\n",
    "    if i == 1:\n",
    "      url = url + \"&page=\" + str(i)\n",
    "    else:\n",
    "      # 첫 페이지가 아닐 때는 뒤에 &from= (10 * (current_page - 1)) 이 붙음\n",
    "      url = url.split(\"&page=\")[0]\n",
    "      url = url + \"&page=\" + str(i) + f\"&from={10 * (i-1)}\"\n",
    "    driver.get(url)\n",
    "    page_source = driver.page_source\n",
    "    print(url)\n",
    "    soup = BeautifulSoup(page_source, 'lxml')\n",
    "    div_list = soup.findAll(\"div\", {\"class\":\"cnn-search__result-contents\"})\n",
    "    for d in div_list:\n",
    "      try:\n",
    "        # 제목, 링크, 콘텐츠, 날짜 값 저장\n",
    "        href = d.find(\"h3\", {\"class\":\"cnn-search__result-headline\"}).find('a').attrs['href']\n",
    "        title = d.find(\"h3\", {\"class\":\"cnn-search__result-headline\"}).find('a').text.strip()\n",
    "        date = d.find(\"div\", {\"class\" : \"cnn-search__result-publish-date\"}).findAll('span')[1].text.strip()\n",
    "        content = d.find(\"div\", {\"class\":\"cnn-search__result-body\"}).text.strip()\n",
    "        href_list.append(href)\n",
    "        title_list.append(title)\n",
    "        date_list.append(date)\n",
    "        content_list.append(content)\n",
    "      except:\n",
    "        continue\n",
    "    url = url[:-1]\n",
    "    # 약간의 시간차를 둠\n",
    "    time.sleep(2)\n",
    "  show_data(href_list, title_list, date_list, content_list)\n",
    "  print(f\"{q} end\")\n",
    "# end of driver\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use multiprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawling(q, driver):\n",
    "  href_list, title_list, date_list, content_list = [], [], [], []\n",
    "  driver = set_driver()\n",
    "  url = f\"https://edition.cnn.com/search/?size=10&q={q}&page=\"\n",
    "  # 아래 range를 변형해서 더 많은 데이터를 수집할 수 있음\n",
    "  for i in range(1, 4):\n",
    "    # 처음일 때와 아닐 때 url 값이 바뀌어서 flag를 이용해 변환\n",
    "    if i == 1:\n",
    "      url = url + \"&page=\" + str(i)\n",
    "    else:\n",
    "      # 첫 페이지가 아닐 때는 뒤에 &from= (10 * (current_page - 1)) 이 붙음\n",
    "      url = url.split(\"&page=\")[0]\n",
    "      url = url + \"&page=\" + str(i) + f\"&from={10 * (i-1)}\"\n",
    "    driver.get(url)\n",
    "    page_source = driver.page_source\n",
    "    print(url)\n",
    "    soup = BeautifulSoup(page_source, 'lxml')\n",
    "    div_list = soup.findAll(\"div\", {\"class\":\"cnn-search__result-contents\"})\n",
    "    for d in div_list:\n",
    "      try:\n",
    "        # 제목, 링크, 콘텐츠, 날짜 값 저장\n",
    "        href = d.find(\"h3\", {\"class\":\"cnn-search__result-headline\"}).find('a').attrs['href']\n",
    "        title = d.find(\"h3\", {\"class\":\"cnn-search__result-headline\"}).find('a').text.strip()\n",
    "        date = d.find(\"div\", {\"class\" : \"cnn-search__result-publish-date\"}).findAll('span')[1].text.strip()\n",
    "        content = d.find(\"div\", {\"class\":\"cnn-search__result-body\"}).text.strip()\n",
    "        href_list.append(href)\n",
    "        title_list.append(title)\n",
    "        date_list.append(date)\n",
    "        content_list.append(content)\n",
    "      except:\n",
    "        continue\n",
    "    url = url[:-1]\n",
    "    # 약간의 시간차를 둠\n",
    "    time.sleep(2)\n",
    "  show_data(href_list, title_list, date_list, content_list)\n",
    "  print(f\"{q} end\")\n",
    "  # end of driver\n",
    "  driver.close()\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = []\n",
    "for query in search_list:\n",
    "    proc = Process(target=crawling, args=(query, driver))\n",
    "    jobs.append(proc)\n",
    "    proc.start()\n",
    "# 모든 multi가 돌아갈 때까지 기다릴 수 있도록 join\n",
    "for p in jobs:\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('soojin': conda)",
   "language": "python",
   "name": "python38564bitsoojinconda500ccdc350d846ea9d3ed9887713270b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
